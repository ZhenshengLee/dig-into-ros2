# 概述

zero-copy通信可以减少通信开销, 降低时延.<br />zero-copy的实现方式有两种, 一种是使用zero-copy的DDS, 这样在通信时, 由中间件保证数据传输是零拷贝的. 这种情况下, 多进程间的通信是零拷贝.<br />iceroxy-dds可以实现, 但是已经停止开发, 这个机制有一个缺点, 就是不能够多机通信. 因为不可能在多主机之间使用共享内存<br />另外一种机制就是进程内通信, 即线程.<br />本文讲述的就是ros2中的进程内节点机制, 与ros1中的nodelet的目标是一样的.<br />The subscriptions and publications mechanisms in ROS 2 fall in two categories:

- intra-process: messages are sent from a publisher to subscriptions via in-process memory.
- inter-process: messages are sent via the underlying ROS 2 middleware layer. The specifics of how this happens depend on the chosen middleware implementation and may involve serialization steps.

# 背景

ROS系统以节点作为资源最小分配单位，这样可以做到模块解耦，提升了容错性，提升开发效率，更好的模块化，更好的代码复用。但是节点机制也会造成性能开销，比如视觉应用中，图像和点云数据在进程间通信往往导致性能瓶颈。ROS1可以在节点内部包含节点，是nodelet机制。在ROS2中我们希望提升nodelets的设计，重构节点，解决一些基础性的问题<br />在这个demo中我们重点演示节点如何手动组织起来。单独定义节点，在不同的进程视图中将其组织起来，而不需要修改节点的代码。

# Intra-process原理

Even if ROS 2 supports intra-process communication, the implementation of this mechanism has still much space for improvement.<br />这个作者对ROS2社区贡献比较多，设计的机制应该已经合入主分支

## 旧机制

The current implementation is based on the creation of a ring buffer for each Publisher and on the publication of meta-messages through the middleware layer. When a Publisher has to publish intra-process, it will pass the message to the IntraProcessManager. Here the message will be stored in the ring buffer associated with the Publisher. In order to extract a message from the IntraProcessManager two pieces of information are needed: the id of the Publisher (in order to select the correct ring buffer) and the position of the message within its ring buffer. A meta-message with this information is created and sent through the ROS 2 middleware to all the Subscriptions, which can then retrieve the original message from the IntraProcessManager.<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/23125517/1664347161260-7081c7b5-f7e1-4c2f-a774-492ba3f7166c.png#averageHue=%23e5e1d7&clientId=u943d20f6-8114-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=ufb0e30c6&margin=%5Bobject%20Object%5D&name=image.png&originHeight=447&originWidth=823&originalType=url&ratio=1&rotation=0&showTitle=false&size=57969&status=done&style=none&taskId=u1416ef49-12b2-4ba6-9055-d1391216933&title=)<br />有很多缺陷

- 无法处理Transient Local的qos
- 依赖RMW层次
- 过大的内存开销
- 过大的延迟和CPU占用
- 当intra-inter-sub时出现问题

## 新机制(dashing)

The new proposal for intra-process communication addresses the issues previously mentioned. It has been designed with performance in mind, so it avoids any communication through the middleware between nodes in the same process.<br />新机制完全避免通过dds进行intra-process，所以没有用上dds的intra机制<br />Consider a simple scenario, consisting of Publishers and Subscriptions all in the same process and with the durability QoS set to volatile. The proposed implementation creates one buffer per Subscription. When a message is published to a topic, its Publisher pushes the message into the buffer of each of the Subscriptions related to that topic and raises a notification, waking up the executor. The executor can then pop the message from the buffer and trigger the callback of the Subscription.<br />每个sub都有一个buffer，pub给sub通知，sub收到通知后通知executor，executor从buffer中取出消息，出发callback<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/23125517/1664347429459-4db4a64a-ad9a-4dfa-8deb-1d1861411d0c.png#averageHue=%23eee2db&clientId=u943d20f6-8114-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=ub1a6ad9a&margin=%5Bobject%20Object%5D&name=image.png&originHeight=569&originWidth=796&originalType=url&ratio=1&rotation=0&showTitle=false&size=64982&status=done&style=none&taskId=u0500745b-75a0-47ef-8b46-f2f36fb49fd&title=)<br />The choice of having independent buffers for each Subscription leads to the following advantages:

- It is easy to support different QoS for each Subscription, while, at the same time, simplifying the implementation.
- 支持不同的sub-qos
- Multiple Subscriptions can extract messages from their own buffer in parallel without blocking each other, thus providing an higher throughput.
- 并行处理，减少阻塞

The only drawback is that the system is not reusing as much resources as possible, compared to sharing buffers between entities. However, from a practical point of view, the memory overhead caused by the proposed implementation with respect to the current one, will always be only a tiny delta compared to the overall memory usage of the application.

### buffer-data-type选择

**看回调的类型**<br />**The choice of the buffer data-type is controlled through an additional field in the SubscriptionOptions.**The default value for this option is denominated CallbackDefault, which corresponds to selecting the type between shared_ptr<constMessageT> and unique_ptr<MessageT> **that better fits with its callback type**. This is deduced looking at the output of AnySubscriptionCallback::use_take_shared_method().<br />buffer中一般存储指针而不是对象本身<br />If the history QoS is set to keep all, the buffers are dynamically adjusted in size up to the maximum resource limits specified by the underlying middleware. On the other hand, if the history QoS is set to keep last, the buffers have a size equal to the depth of the history and they act as ring buffers (overwriting the oldest data when trying to push while its full).<br />Note that in case of publishers with keep all and reliable communication, the behavior can be different from the one of inter-process communication. In the inter-process case, the middlewares use buffers in both publisher and subscription. If the subscription queue is full, the publisher one would start to fill and then finally the publish call would block when that queue is full. Since the intra-process communication uses a single queue on the subscription, this behavior can’t be exactly emulated.<br />Buffers are not only used in Subscriptions but also in each Publisher with a durability QoS of type transient local. The data-type stored in the Publisher buffer is always shared_ptr<const MessageT>.<br />A new class derived from rclcpp::Waitable is defined, which is named SubscriptionIntraProcessWaitable. An object of this type is created by each Subscription with intra-process communication enabled and it is used to notify the Subscription that a new message has been pushed into its ring buffer and that it needs to be processed.<br />通知机制采用waitable，继承于rclcpp::Waitable<br />The IntraProcessManager class stores information about each Publisher and each Subscription, together with pointers to these structures. This allows the system to know which entities can communicate with each other and to have access to methods for pushing data into the buffers.<br />**The decision whether to publish inter-process, intra-process or both is made every time the Publisher::publish() method is called. For example, if the NodeOptions::use_intra_process_comms_ is enabled and all the known Subscriptions are in the same process, then the message is only published intra-process. This remains identical to the current implementation.**<br />**注意：每次publish的时候都会判断是否走intra流程，条件包括**

- node_option中use_intra_process_comms
- subscription与publisher在同一个进程中

# Intra-process-流程

### Creating a publisher

1. User calls Node::create_publisher<MessageT>(...).
2. This boils down to NodeTopics::create_publisher(...), where a Publisher is created through the factory.
3. Here, if intra-process communication is enabled, eventual intra-process related variables are initialized through the Publisher::SetupIntraProcess(...) method.
4. Then the IntraProcessManager is notified about the existence of the new Publisher through the method IntraProcessManager::add_publisher(PublisherBase::SharedPtr publisher, PublisherOptions options).
5. IntraProcessManager::add_publisher(...) stores the Publisher information in an internal structure of type PublisherInfo. The structure contains information about the Publisher, such as its QoS and its topic name, and a weak pointer for the Publisher object. An uint64_t pub_id unique within the rclcpp::Context is assigned to the Publisher. The IntraProcessManager contains a std::map<uint64_t, PublisherInfo> object where it is possible to retrieve the PublisherInfo of a specific Publisher given its id. The function returns the pub_id, that is stored within the Publisher.

If the Publisher QoS is set to transient local, then the Publisher::SetupIntraProcess(...) method will also create a ring buffer of the size specified by the depth from the QoS.

### Creating a subscription

1. User calls Node::create_subscription<MessageT>(...).
2. This boils down to NodeTopics::create_subscription(...), where a Subscription is created through the factory.
3. Here, if intra-process communication is enabled, intra-process related variables are initialized through the Subscription::SetupIntraProcess(...) method. The most relevant ones being the ring buffer and the waitable object.
4. Then the IntraProcessManager is notified about the existence of the new Subscription through the method IntraProcessManager::add_subscription(SubscriptionBase::SharedPtr subscription, SubscriptionOptions options).
5. IntraProcessManager::add_subscription(...) stores the Subscription information in an internal structure of type SubscriptionInfo. The structure contains information about the Subscription, such as its QoS, its topic name and the type of its callback, and a weak pointer for the Subscription object. An uint64_t sub_id unique within the rclcpp::Context is assigned to the Subscription. The IntraProcessManager contains a std::map<uint64_t, SubscriptionInfo> object where it is possible to retrieve the SubscriptionInfo of a specific Subscription given its id. There is also an additional structure std::map<uint64_t, std::pair<std::set<uint64_t>, std::set<uint64_t>>>. The key of the map is the unique id of a Publisher and the value is a pair of sets of ids. These sets contain the ids of the Subscriptions that can communicate with the Publisher. We have two different sets because we want to differentiate the Subscriptions depending on whether they request ownership of the received messages or not (note that this decision is done looking at their buffer, since the Publisher does not have to interact with the Subscription callback).
6. The SubscriptionIntraProcessWaitable object is added to the list of Waitable interfaces of the node through node_interfaces::NodeWaitablesInterface::add_waitable(...). It is added to the same callback group used for the standard inter-process communication of that topic.

### Publishing only intra-process

#### Publishing unique_ptr

1. User calls Publisher::publish(std::unique_ptr<MessageT> msg).
2. Publisher::publish(std::unique_ptr<MessageT> msg) calls IntraProcessManager::do_intra_process_publish(uint64_t pub_id, std::unique_ptr<MessageT> msg).
3. IntraProcessManager::do_intra_process_publish(...) uses the uint64_t pub_id to call IntraProcessManager::get_subscription_ids_for_pub(uint64_t pub_id). This returns the ids corresponding to Subscriptions that have a QoS compatible for receiving the message. These ids are divided into two sublists, according to the data-type that is stored in the buffer of each Susbscription: requesting ownership (unique_ptr<MessageT>) or accepting shared (shared_ptr<MessageT>, but also MessageT since it will copy data in any case).
4. The message is “added” to the ring buffer of all the items in the lists. The rcl_guard_condition_t member of SubscriptionIntraProcessWaitable of each Subscription is triggered (this wakes up rclcpp::spin).

The way in which the std::unique_ptr<MessageT> message is “added” to a buffer, depends on the type of the buffer.

- BufferT = unique_ptr<MessageT> The buffer receives a copy of MessageT and has ownership on it; for the last buffer, a copy is not necessary as ownership can be transferred.
- BufferT = shared_ptr<const MessageT> Every buffer receives a shared pointer of the same MessageT; no copies are required.
- BufferT = MessageT A copy of the message is added to every buffer.

![image.png](https://cdn.nlark.com/yuque/0/2022/png/23125517/1664347596850-f720f08d-3638-4bc2-bfd0-7af408870064.png#averageHue=%23f0f0f0&clientId=u943d20f6-8114-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=u81ef79a0&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1041&originWidth=810&originalType=url&ratio=1&rotation=0&showTitle=false&size=107166&status=done&style=none&taskId=u3d52488b-32eb-44dc-a6d7-70bb94795ed&title=)

#### Publishing other message types

The Publisher::publish(...) method is overloaded to support different message types:

- unique_ptr<MessageT>
- MessageT &
- MessageT*
- const shared_ptr<const MessageT>

**The last two of them are actually deprecated since ROS 2 Dashing**. All these methods are unchanged with respect to the current implementation: they end up creating a unique_ptr and calling the Publisher::publish(std::unique_ptr<MessageT> msg) described above.

### Receiving intra-process messages

As previously described, whenever messages are added to the ring buffer of a Subscription, a condition variable specific to the Subscription is triggered. This condition variable has been added to the Node waitset so it is being monitored by the rclcpp::spin.<br />Remember that the SubscriptionIntraProcessWaitable object has access to the ring buffer and to the callback function pointer of its related Subscription.

1. The guard condition linked with the SubscriptionIntraProcessWaitable object awakes rclcpp::spin.
2. The SubscriptionIntraProcessWaitable::is_ready() condition is checked. This has to ensure that the ring buffer is not empty.
3. The SubscriptionIntraProcessWaitable::execute() function is triggered. Here the first message is extracted from the buffer and then the SubscriptionIntraProcessWaitable calls the AnySubscriptionCallback::dispatch_intra_process(...) method. There are different implementations for this method, depending on the data-type stored in the buffer.
4. The AnySubscriptionCallback::dispatch_intra_process(...) method triggers the associated callback. Note that in this step, if the type of the buffer is a smart pointer one, no message copies occurr, as ownership has been already taken into account when pushing a message into the queue.

### Publishing intra and inter-process

1. User calls Publisher::publish(std::unique_ptr<MessageT> msg).
2. The message is moved into a shared pointer std::shared_ptr<MessageT> shared_msg = std::move(msg).
3. Publisher::publish(std::unique_ptr<MessageT> msg) calls IntraProcessManager::do_intra_process_publish(uint64_t pub_id, std::shared_ptr<MessageT> shared_msg).

The following steps are identical to steps 3, 4, and 5 applied when publishing only intra-process.

1. IntraProcessManager::do_intra_process_publish(...) uses the uint64_t pub_id to call IntraProcessManager::get_subscription_ids_for_pub(uint64_t pub_id). Then it calls IntraProcessManager::find_matching_subscriptions(PublisherInfo pub_info). This returns the ids corresponding to Subscriptions that have a QoS compatible for receiving the message. These ids are divided into two sublists, according to the data-type that is stored in the buffer of each Susbscription: requesting ownership (unique_ptr<MessageT>) or accepting shared (shared_ptr<MessageT>, but also MessageT since it will copy data in any case).
2. The message is “added” to the ring buffer of all the items in the list. The rcl_guard_condition_t member of SubscriptionIntraProcessWaitable of each Subscription is triggered (this wakes up rclcpp::spin).

**After the intra-process publication, the inter-process one takes place.**

1. Publisher::publish(std::unique_ptr<MessageT> msg) calls Publisher::do_inter_process_publish(const MessageT & inter_process_msg), where MessageT inter_process_msg = *shared_msg.

**The difference from the previous case is that here a std::shared_ptr<const MessageT> is being “added” to the buffers. Note that this std::shared_ptr has been just created from a std::unique_ptr and it is only used by the IntraProcessManager and by the RMW**, while the user application has no access to it.

- BufferT = unique_ptr<MessageT> The buffer receives a copy of MessageT and has ownership on it.
- BufferT = shared_ptr<const MessageT> Every buffer receives a shared pointer of the same MessageT, so no copies are required.
- BufferT = MessageT A copy of the message is added to every buffer.

**The difference with publishing a unique_ptr is that here it is not possible to save a copy. If you move the ownership of the published message to one of the Subscription (so potentially saving a copy as done in the previous case), you will need to create a new copy of the message for inter-process publication.**<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/23125517/1664347596659-8cd639a0-064c-4815-9755-b3720e03c3dd.png#averageHue=%23f4f3f3&clientId=u943d20f6-8114-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=uef1533ba&margin=%5Bobject%20Object%5D&name=image.png&originHeight=761&originWidth=928&originalType=url&ratio=1&rotation=0&showTitle=false&size=84651&status=done&style=none&taskId=u78f0fe7b-3355-4f69-96f3-912c1e4e415&title=)

### QoS features

The proposed implementation can handle all the different QoS.

- If the history is set to keep_last, then the depth of the history corresponds to the size of the ring buffer. On the other hand, if the history is set to keep_all, the buffer becomes a standard FIFO queue with an unbounded size.
- The reliability is only checked by the IntraProcessManager in order to understand if a Publisher and a Subscription are compatible. The use of buffers ensures that all the messages are delivered without the need to resend them. Thus, both options, reliable and best-effort, are satisfied.
- The durability QoS is used to understand if a Publisher and a Subscription are compatible. How this QoS is handled is described in details in the following paragraph.

#### Handling Transient Local

If the Publisher durability is set to transient_local an additional buffer on the Publisher side is used to store the sent intra-process messages.<br />Late-joiner Subscriptions will have to extract messages from this buffer once they are added to the IntraProcessManager. In this case the IntraProcessManager has to check if the recently created Subscription is a late-joiner, and, if it is, it has to retrieve messages from the Transient Local Publishers.

1. Call IntraProcessManager::find_matching_publishers(SubscriptionInfo sub_info) that returns a list of stored PublisherInfo that have a QoS compatible for sending messages to this new Subscription. These will be all Transient Local Publishers, so they have a ring buffer.
2. Copy messages from all the ring buffers found into the ring buffer of the new Subscription. **TODO:** are there any constraints on the order in which old messages have to be retrieved? (i.e. 1 publisher at the time; all the firsts of each publisher, then all the seconds …).
3. If at least 1 message was present, trigger the rcl_guard_condition_t member of the SubscriptionIntraProcessWaitable associated with the new Subscription.

However, this is not enough as it does not allow to handle the scenario in which a transient local Publisher has only intra-process Subscriptions when it is created, but, eventually, a transient local Subscription in a different process joins. Initially, published messages are not passed to the middleware, since all the Subscriptions are in the same process. This means that the middleware is not able to store old messages for eventual late-joiners.<br />The solution to this issue consists in always publishing both intra and inter-process when a Publisher has transient local durability. For this reason, when transient local is enabled, the do_intra_process_publish(...) function will always process a shared pointer. This allows us to add the logic for storing the published messages into the buffers only in one of the two do_intra_process_publish(...) cases and also it allows to use buffers that have only to store shared pointers.

# Intra-process数据拷贝分析

对于每个数据的拷贝次数，history-depth，意味着数据会暂存到history中<br />In the previous sections, it has been briefly described how a message can be added to a buffer, i.e. if it is necessary to copy it or not.<br />Here some details about how this proposal adresses some more complex cases.<br />As previously stated, regardless of the data-type published by the user, the flow always goes towards Publisher::publish(std::unique_ptr<MessageT> msg).<br />**不管用户发布的是什么类型的指针，最终发布都是unique_ptr**<br />The std::unique_ptr<MessageT> msg is passed to the IntraProcessManger that decides **how to add this message to the buffers**. The decision is taken looking at the **number and the type**, i.e. if they want ownership on messages or not, of the Subscriptions.<br />If all the Subscriptions want ownership of the message, then a total of N-1 copies of the message are required, where N is the number of Subscriptions. **The last one will receive ownership of the published message, thus saving a copy.**<br />**这就是1:1 unique没有copy的原因！**<br />**对于intra的场景，我们需要知道sub需不需要message的ownership，需要操作该数据嘛？一般情况下，sub的操作是，拷贝一次，然后再做处理。**<br />If none of the Subscriptions want ownership of the message, 0 copies are required. It is possible to convert the message into a std::shared_ptr<MessageT> msg and to add it to every buffer.<br />只要有一个sub需要所有权，等同于所有sub都需要所有权，如果所有sub不需要所有权，则可以使用shared_ptr<br />If there is 1 Subscription that does not want ownership while the others want it, the situation is equivalent to the case of everyone requesting ownership:N-1 copies of the message are required. As before the last Subscription will receive ownership.<br />If there is more than 1 Subscription that do not want ownership while the others want it, a total of M copies of the message are required, where M is the number of Subscriptions that want ownership. 1 copy will be shared among all the Subscriptions that do not want ownership, while M-1 copies are for the others.<br />As in the current implementation, if both inter and intra-process communication are needed, the std::unique_ptr<MessageT> msg will be converted into a std::shared_ptr<MessageT> msg and passed respectively to the do_intra_process_publish and do_inter_process_publish functions.<br />如果同时又intra-sub和inter-sub，则unique_ptr会转化为shared_ptr，如果subscription requesting，<br />A copy of the message will be given to all the Subscriptions requesting ownership, while the others can copy the published shared pointer.<br />The following tables show a recap of when the proposed implementation has to create a new copy of a message. **The notation @ indicates a memory address where the message is stored, different memory addresses correspond to different copies of the message.**

#### Publishing UniquePtr

| publish<T> | BufferT | Results |
| --- | --- | --- |
| **unique_ptr<MsgT> @1** | **unique_ptr<MsgT>** | **@1** |
| **unique_ptr<MsgT> @1** | **unique_ptr<MsgT><br />unique_ptr<MsgT>** | **@1<br />@2** |
| **unique_ptr<MsgT> @1** | **shared_ptr<MsgT>** | **@1** |
| **unique_ptr<MsgT> @1** | **shared_ptr<MsgT><br />shared_ptr<MsgT>** | **@1<br />@1** |
| unique_ptr<MsgT> @1 | unique_ptr<MsgT><br />shared_ptr<MsgT> | @1<br />@2 |
| unique_ptr<MsgT> @1 | unique_ptr<MsgT><br />shared_ptr<MsgT><br />shared_ptr<MsgT> | @1<br />@2<br />@2 |
| unique_ptr<MsgT> @1 | unique_ptr<MsgT><br />unique_ptr<MsgT><br />shared_ptr<MsgT><br />shared_ptr<MsgT> | @1<br />@2<br />@3<br />@3 |

#### Publishing SharedPtr

| publish<T> | BufferT | Results |
| --- | --- | --- |
| **shared_ptr<MsgT> @1** | **unique_ptr<MsgT>** | **@2** |
| shared_ptr<MsgT> @1 | unique_ptr<MsgT><br />unique_ptr<MsgT> | @2<br />@3 |
| **shared_ptr<MsgT> @1** | **shared_ptr<MsgT>** | **@1** |
| **shared_ptr<MsgT> @1** | **shared_ptr<MsgT><br />shared_ptr<MsgT>** | **@1<br />@1** |
| shared_ptr<MsgT> @1 | unique_ptr<MsgT><br />shared_ptr<MsgT> | @2<br />@1 |
| shared_ptr<MsgT> @1 | unique_ptr<MsgT><br />shared_ptr<MsgT><br />shared_ptr<MsgT> | @2<br />@1<br />@1 |
| shared_ptr<MsgT> @1 | unique_ptr<MsgT><br />unique_ptr<MsgT><br />shared_ptr<MsgT><br />shared_ptr<MsgT> | @2<br />@3<br />@1<br />@1 |

The possibility of setting the data-type stored in each buffer becomes helpful when dealing with more particular scenarios.<br />Considering a scenario with N Subscriptions all taking a unique pointer. If the Subscriptions don’t actually take the message (e.g. they are busy and the message is being overwritten due to QoS settings) the default buffer type (**unique_ptr since the callbacks require ownership**) would result in the copy taking place anyway. By setting the buffer type to shared_ptr, no copies are needed when the Publisher pushes messages into the buffers. Eventually, the Subscriptions will copy the data only when they are ready to process it.<br />On the other hand, if the published data are very small, it can be advantageous to do not use C++ smart pointers, but to directly store the data into the buffers.<br />**如果数据很小，则可以用messageType本身**<br />In all this situations, the number of copies is always smaller or equal than the one required for the current intra-process implementation.<br />**However, there is a particular scenario where having multiple buffers makes much more difficult saving a copy. There are two Subscriptions, one taking a shared pointer and the other taking a unique pointer. With a more centralized system, if the first Subscription requests its shared pointer and then releases it before the second Subscription takes the message, it is potentially possible to optimize the system to manage this situation without requiring any copy. On the other hand, the proposed implementation will immediately create one copy of the message for the Subscription requiring ownership. Even in case of using a shared_ptr buffer as previously described, it becomes more difficult to ensure that the other Subscription is not using the pointer anymore.**

#### Where are these copies performed?

The IntraProcessManger::do_intra_process_publish(...) function**knows whether the intra-process buffer of each Subscription requires ownership or not**. For this reason it can perform the minimum number of copies required by looking at the total number of Subscriptions and their types. **The buffer does not perform any copy when receiving a message, but directly stores it.**<br />When extracting a message from the buffer, the Subscription can require any particular data-type. **The intra-process buffer will perform a copy of the message whenever necessary**, for example in the previously described cases where the data-type stored in the buffer is different from the callback one.<br />buffer会根据回调的类型，做必要的拷贝，shared_ptr不需要拷贝

# Intra-process-demo

## inter-node-intra-process

producer-somsumer<br />这个demo用来演示两个进程内节点间可以通过std::unique_ptr来传递数据，实现零拷贝。

```cpp
int main(int argc, char * argv[])
{
  setvbuf(stdout, NULL, _IONBF, BUFSIZ);
  rclcpp::init(argc, argv);
  rclcpp::executors::SingleThreadedExecutor executor;

  auto producer = std::make_shared<Producer>("producer", "number");
  auto consumer = std::make_shared<Consumer>("consumer", "number");

  executor.add_node(producer);
  executor.add_node(consumer);
  executor.spin();

  rclcpp::shutdown();

  return 0;
}
```

通过打印可以看到地址是一致的<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/23125517/1664349640124-4ea1d1d9-8ad3-44bf-9479-eb10c3ed4db7.png#averageHue=%23edfcc8&clientId=u943d20f6-8114-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=320&id=ub15d7c46&margin=%5Bobject%20Object%5D&name=image.png&originHeight=320&originWidth=718&originalType=binary&ratio=1&rotation=0&showTitle=false&size=45835&status=done&style=none&taskId=uc02dbf68-9aeb-4894-84f3-30cd313fefe&title=&width=718)<br />**要求发布和订阅都是unique_ptr**<br />Finally, you can see that “Published message…” and “Received message …” lines with the same value also have the same address. This shows that the address of the message being received is the same as the one that was published and that it is not a copy. **This is because we’re publishing and subscribing with std::unique_ptrs which allow ownership of a message to be moved around the system safely.** **You can also publish and subscribe with const & and std::shared_ptr, but zero-copy will not occur in that case.**<br />**所以unique_ptr是requesting ownership**

## intra-node-intra-process

cyclic pipeline demo循环流水线<br />这个例子和前一个例子差不多，但是producer只产生一次数据，然后在comsumer和producer之间循环处理<br />This is achieved by creating a cycle in the graph and “kicking off” communication by externally making one of the nodes publish before spinning the executor:

```cpp
int main(int argc, char * argv[])
{
  setvbuf(stdout, NULL, _IONBF, BUFSIZ);
  rclcpp::init(argc, argv);
  rclcpp::executors::SingleThreadedExecutor executor;

  // Create a simple loop by connecting the in and out topics of two IncrementerPipe's.
  // The expectation is that the address of the message being passed between them never changes.
  auto pipe1 = std::make_shared<IncrementerPipe>("pipe1", "topic1", "topic2");
  auto pipe2 = std::make_shared<IncrementerPipe>("pipe2", "topic2", "topic1");
  rclcpp::sleep_for(1s); // Wait for subscriptions to be established to avoid race conditions.
  // Publish the first message (kicking off the cycle).
  std::unique_ptr<std_msgs::msg::Int32> msg(new std_msgs::msg::Int32());
  msg->data = 42;
  printf(
    "Published first message with value: %d, and address: 0x%" PRIXPTR "\n", msg->data,
    reinterpret_cast<std::uintptr_t>(msg.get()));
  pipe1->pub->publish(std::move(msg));

  executor.add_node(pipe1);
  executor.add_node(pipe2);
  executor.spin();

  rclcpp::shutdown();

  return 0;
}
```

这个没有定义两个节点，而是定义一个节点，实例化两个<br />这个节点即发布消息也订阅消息<br />这样实现了循环处理The graph ends up being pipe1 -> pipe2 -> pipe1 … in a loop.<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/23125517/1664349653668-0c2e409c-a4ee-4bac-8cf1-fe958300af44.png#averageHue=%23edfdc9&clientId=u943d20f6-8114-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=503&id=u553f7d12&margin=%5Bobject%20Object%5D&name=image.png&originHeight=503&originWidth=702&originalType=binary&ratio=1&rotation=0&showTitle=false&size=56603&status=done&style=none&taskId=ub691dd05-b505-4b0d-a34a-76be73f6b49&title=&width=702)<br />**要求发布和订阅都是unique_ptr**

## intra-single-sub的image_pipline

进程内单订阅节点简单流水线<br />First we’ll have a pipeline of three nodes, arranged as such: camera_node -> watermark_node -> image_view_node<br />相机节点读取相机数据，往图像信息中写入数据然后发布，水印节点订阅图像，进行水印处理，图像显示节点订阅图像，使用imshow显示出来，这三个节点都在一个进程中。<br />In each node the address of the message which is being sent, or which has been received, or both, is written to the image. The watermark and image view nodes are designed to modify the image without copying it and so the addresses imprinted on the image should all be the same as long as the nodes are in the same process and the graph remains organized in a pipeline as sketched above.<br />订阅节点设计为只是修改图像，而不是拷贝图像，所以不会产生额外的拷贝<br />如果修改图像后传入GPU，则一定存在拷贝<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/23125517/1664349675999-cc02475e-139f-4f58-85c3-cc622c915d18.png#averageHue=%233d9b3a&clientId=u943d20f6-8114-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=456&id=u46129d37&margin=%5Bobject%20Object%5D&name=image.png&originHeight=456&originWidth=599&originalType=binary&ratio=1&rotation=0&showTitle=false&size=289830&status=done&style=none&taskId=ucca1178c-5848-43c8-beb3-ce28bb4ac21&title=&width=599)<br />三行分别是camera_node发布前写上去的image地址<br />watermark收到并写上去的地址<br />image_view收到并写上去的地址

## intra-multi-sub的image_pipline

进程内多个订阅者<br />![](https://cdn.nlark.com/yuque/0/2022/png/23125517/1664333106432-0a40335f-e9cc-40aa-9215-55c09dfc23ff.png#averageHue=%23f6f3f0&clientId=uc51f4d69-e238-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=u20fd1ac7&margin=%5Bobject%20Object%5D&originHeight=352&originWidth=647&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u32af2475-7486-46d0-b6d5-4f5317033bf&title=)<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/23125517/1664352010719-5e8d6ccd-4a58-42a1-bf23-263e0a57328a.png#averageHue=%23b4e09d&clientId=u2743ff1a-dff1-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=368&id=u4065016c&margin=%5Bobject%20Object%5D&name=image.png&originHeight=368&originWidth=728&originalType=binary&ratio=1&rotation=0&showTitle=false&size=305030&status=done&style=none&taskId=u443d6eac-b095-4cdd-9ad9-4e1f0ec9ccd&title=&width=728)<br />**不看两张图片上的地址是否一致，而是看一张图片上的三个地址是否一致**<br />**第二张图的第三个地址为什么不一致？发生了什么？**<br />**However, there is a particular scenario where having multiple buffers makes much more difficult saving a copy. There are two Subscriptions, one taking a shared pointer and the other taking a unique pointer. With a more centralized system, if the first Subscription requests its shared pointer and then releases it before the second Subscription takes the message, it is potentially possible to optimize the system to manage this situation without requiring any copy. On the other hand, the proposed implementation will immediately create one copy of the message for the Subscription requiring ownership. Even in case of using a shared_ptr buffer as previously described, it becomes more difficult to ensure that the other Subscription is not using the pointer anymore.**<br />可能是因为这个，没弄懂。。。<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/23125517/1664361080452-bf35fe43-ed52-464e-a874-beda93c13243.png#averageHue=%23f7f2ef&clientId=u9d6f6271-9cb4-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=531&id=u2f3764f8&margin=%5Bobject%20Object%5D&name=image.png&originHeight=531&originWidth=693&originalType=binary&ratio=1&rotation=0&showTitle=false&size=104875&status=done&style=none&taskId=u5a22a65a-d3a8-492e-bbe4-7fa9c9922bb&title=&width=693)<br />此时很有趣，如果是unique_ptr的话，则view_node2受到的数据是经过拷贝的，如果是shared_ptr，则没有拷贝<br />The link between the camera_node and the watermark_node can use the same pointer without copying because there is only one intra process subscription to which the message should be delivered. But for the link between the watermark_node and the two image view nodes the relationship is one to many, so if the image view nodes were using unique_ptr callbacks then it would be impossible to deliver the ownership of the same pointer to both. It can be, however, delivered to one of them. Which one would get the original pointer is not defined, but instead is simply the last to be delivered.<br />如果是unique_ptr，则无法共享所有权，则两个订阅者必须将回调定义为shared_ptr（在图片中是推断不出这个结论的）<br />Note that the image view nodes are not subscribed with unique_ptr callbacks. Instead they are subscribed with const shared_ptrs. **This means the system deliveres the same shared_ptr to both callbacks. When the first intraprocess subscription is handled, the internally stored unique_ptr is promoted to a shared_ptr. Each of the callbacks will receive shared ownership of the same message.**<br />**intra_process_manager在发送的时候将unique_ptr转换成shared_ptr**

## inter-sub的image_pipline

One other important thing to get right is to avoid interruption of the intra process zero-copy behavior when interprocess subscriptions are made. To test this we can run the first image pipeline demo, image_pipeline_all_in_one, and then run an instance of the stand alone image_view_node (don’t forget to prefix them with ros2 run intra_process_demo in the terminal). This will look something like this:<br />进程内进程间混合订阅的情况，inter-sub不会影响intra-sub的零拷贝特性<br />Pipeline with interprocess viewer跨进程viewer的情况<br />![image.png](https://cdn.nlark.com/yuque/0/2022/png/23125517/1664357318168-65ecca7d-e42e-4c27-b237-b3671838a185.png#averageHue=%2392887a&clientId=u9d6f6271-9cb4-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=450&id=u04b33de1&margin=%5Bobject%20Object%5D&name=image.png&originHeight=450&originWidth=719&originalType=binary&ratio=1&rotation=0&showTitle=false&size=343972&status=done&style=none&taskId=u9dd1edd0-c343-4c4c-ae3a-f5c0b21874d&title=&width=719)<br />**不看两张图片上的地址是否一致，而是看一张图片上的三个地址是否一致**<br />跨进程情况下，进程内部没有拷贝，进程外部是有拷贝的！

# Intra-process性能

The implementation of the presented new intra-process communication mechanism is hosted on [GitHub here](https://github.com/alsora/rclcpp/tree/alsora/new_ipc_proposal).<br />This section contains experimental results obtained comparing the current intra-process communication implementation with an initial implementation of the proposed one. The tests span multiple ROS 2 applications and use-cases and have been validated on different machines.<br />All the following experiments have been run using the ROS 2 Dashing and with -O2 optimization enabled.<br />colcon build --cmake-args  -DCMAKE_CXX_FLAGS="-O2" -DCMAKE_C_FLAGS="-O2" <br />The first test has been carried out using the intra_process_demo package contained in the [ROS 2 demos repository](https://github.com/ros2/demos). A first application, called image_pipeline_all_in_one, is made of 3 nodes, where the fist one publishes a unique_ptr<Image> message. A second node subscribes to the topic and republishes the image after modifying it on a new topic. A third node subscribes to to this last topic.<br />Also a variant of the application has been tested: it’s image_pipeline_with_two_image_view, where there are 2 consumers at the end of the pipeline.<br />In these tests the latency is computed as the total pipeline duration, i.e. the time from when the first node publishes the image to when the last node receives it. The CPU usage and the latency have been obtained from top command and averaged over the experiment duration.<br />Performance evaluation on a laptop computer with Intel i7-6600U CPU @ 2.60GHz.

| ROS 2 system | IPC | RMW | Latency [us] | CPU [%] | RAM [Mb] |
| --- | --- | --- | --- | --- | --- |
| image_pipeline_all_in_one | off | Fast-RTPS | 1800 | 23 | 90 |
| image_pipeline_all_in_one | standard | Fast-RTPS | 920 | 20 | 90 |
| image_pipeline_all_in_one | new | Fast-RTPS | 350 | 15 | 90 |
| image_pipeline_with_two_image_view | off | Fast-RTPS | 2900 | 24 | 94 |
| image_pipeline_with_two_image_view | standard | Fast-RTPS | 2000 | 20 | 95 |
| image_pipeline_with_two_image_view | new | Fast-RTPS | 1400 | 16 | 94 |

From this simple experiment is immediately possible to see the improvement in the latency when using the proposed intra-process communication. However, an even bigger improvement is present when analyzing the results from more complex applications.<br />The next results have been obtained running the iRobot benchmark application. This allows the user to specify the topology of a ROS 2 graph that will be entirely run in a single process.<br />The application has been run with the topologies Sierra Nevada and Mont Blanc. Sierra Nevada is a 10-node topology and it contains 10 publishers and 13 subscriptions. One topic has a message size of 10KB, while all the others have message sizes between 10 and 100 bytes.<br />Mont Blanc is a bigger 20-node topology, containing 23 publishers and 35 subscriptions. Two topics have a message size of 250KB, three topics have message sizes between 1KB and 25KB, and the rest of the topics have message sizes smaller than 1KB.<br />A detailed description and the source code for these application and topologies can be found [here](https://github.com/irobot-ros/ros2-performance/tree/master/performances/benchmark).<br />Note that, differently from the previous experiment where the ownership of the messages was moved from the publisher to the subscription, here nodes use const std::shared_ptr<const MessageT> messages for the callbacks.<br />Performance evaluation on a laptop computer with Intel i7-6600U CPU @ 2.60GHz.

| ROS 2 system | IPC | RMW | Latency [us] | CPU [%] | RAM [Mb] |
| --- | --- | --- | --- | --- | --- |
| Sierra Nevada | off | Fast-RTPS | 600 | 14 | 63 |
| Sierra Nevada | standard | Fast-RTPS | 650 | 16 | 73->79 |
| Sierra Nevada | new | Fast-RTPS | 140 | 8 | 63 |
| Mont Blanc | off | Fast-RTPS | 1050 | 22 | 180 |
| Mont Blanc | standard | Fast-RTPS | 750 | 18 | 213->220 |
| Mont Blanc | new | Fast-RTPS | 160 | 8 | 180 |

A similar behavior can be observed also running the application on resource constrained platforms. The following results have been obtained on a RaspberryPi 2.

| ROS 2 system | IPC | RMW | Latency [us] | CPU [%] | RAM [Mb] |
| --- | --- | --- | --- | --- | --- |
| Sierra Nevada | off | Fast-RTPS | 800 | 18 | 47 |
| Sierra Nevada | standard | Fast-RTPS | 725 | 20 | 54->58 |
| Sierra Nevada | new | Fast-RTPS | 170 | 10 | 47 |
| Mont Blanc | off | Fast-RTPS | 1500 | 30 | 130 |
| Mont Blanc | standard | Fast-RTPS | 950 | 26 | 154->159 |
| Mont Blanc | new | Fast-RTPS | 220 | 14 | 130 |

For what concerns latency and CPU usage, Sierra Nevada behaves almost the same regardless if standard IPC is enabled or not. This is due to the fact that most of its messages are very small in size. On the other hand, there are noticeable improvements in Mont Blanc, where several messages of non-negligible size are used.<br />From the memory point of view, there is an almost constant increase in the utilization during the execution of the program when standard intra-process communication mechanism is used. Since the experiments have been run for 120 seconds, there is an increase of approximately 60KB per second. However, even considering the initial memory usage, it is possible to see how it is affected from the presence of the additional publishers and subscriptions needed for intra-process communication. There is a difference of 10MB in Sierra Nevada and of 33MB in Mont Blanc between standard intra-process communication on and off.<br />The last experiment show how the current implementation performs in the case that both intra and inter-process communication are needed. The test consists of running Sierra Nevada on RaspberryPi 2, and, in a separate desktop machine, a single node subscribing to all the available topics coming from Sierra Nevada. This use-case is common when using tools such as rosbag or rviz.

| ROS 2 system | IPC | RMW | Latency [us] | CPU [%] | RAM [Mb] |
| --- | --- | --- | --- | --- | --- |
| Sierra Nevada + debug node | off | Fast-RTPS | 800 | 22 | 50 |
| Sierra Nevada + debug node | standard | Fast-RTPS | 1100 | 35 | 60->65 |
| Sierra Nevada + debug node | new | Fast-RTPS | 180 | 15 | 32 |

These results show that if there is at least one node in a different process, with the current implementation it is better to keep intra-process communication disabled. The proposed implementation does not require the ROS 2 middleware when publishing intra-process. This allows to easily remove the connections between nodes in the same process when it is required to publish also inter process, potentially resulting in a very small overhead with respect to the only intra-process case.

# 已知问题

There are some open issues that are not addressed neither on the current implementation nor on the proposed one.

- The proposal does not take into account the problem of having a queue with twice the size when both inter and intra-process communication are used. A Publisher or a Subscription with a history depth of 10 will be able to store up to 20 messages without processing them (10 intra-process and 10 inter-process). This issue is also present in the current implementation, since each Subscription is doubled.
  - 本质是因为intra-manager实现了自己的qos处理机制

# 讨论

## buffer中是否一定是ptr

如果数据很小，是内置数据类型，则发送数据本身是更好的选择<br />不过这种场景不多，intra-process一般用于图像零拷贝传输

## 拷贝由谁做？

intra-process-manager做，而且这个拷贝只在内部使用，用户无法访问，专用于buffer，和middleware<br />buffer也会做拷贝，主要是根据回调的类型，如果是unique_ptr，则一般要拷贝

## 拷贝次数的计算

### pub发送的是什么ptr?

用户决定，不管用户发送什么ptr，都会转成unique_ptr，Intra-process要求发送unique_ptr<br />如果是纯intra-process，则是unique_ptr，如果intra-inter-sub，则发送后会转换成shared_ptr<br />其实就是判断是否是纯intra, 是纯的就是unique_ptr

### buffer中的是什么ptr？

总的原则是根据sub的callback函数签名，要么是unique，要么是shared？<br />用户怎么决定用什么呢？怎么判断sub是否requesting ownership?<br />**所以unique_ptr是requesting ownership**

### 1:1unique_ptr为什么无拷贝？

这一个可以直接move给你<br />If all the Subscriptions want ownership of the message, then a total of N-1 copies of the message are required, where N is the number of Subscriptions. **The last one will receive ownership of the published message, thus saving a copy.**

## Intra传输机制是否走dds?

dds也有intra-process机制，但是rclcpp没有用这个机制，而是走自己的intra-manager，没必要走dds<br />不走dds，带来两个结果

- 跟dds规范有关的机制，比如qos，需要单独实现处理机制，这些是由IntraProcessManager 来完成的
  - 不过都已经是intra了，还要什么qos呢，至少不需要特别复杂的qos
  - 但是，inter-sub的情况，还是需要qos，inter-sub的情况其实特别常见，毕竟ros2 topic echo就是。
  - intra-multi-sub同样需要qos?
- 不需要serialization
  - 节省了拷贝
  - 同样inter-sub的情况还是走dds

## Intra与zc性能对比

在多个sub的情况下，依然会有拷贝，而zc在多个sub的情况下不会有拷贝

## Intra-process与composition的关系

没有强耦合关系，要实现intra-process只需要在node_option中设置即可

```cpp
rclcpp::NodeOptions().use_intra_process_comms(true)
```

composition和component机制更多是为了模块化设计，方便以脚本的形式设计每个进程的运行布局，进行程序加载（特别是运行时刻的动态加载和卸载）<br />方便控制运行布局，也意味着可以方便选择是将多个节点部署在单进程还是多进程，实现高效通信<br />By making the process layout a deploy-time decision the user can choose between:

- running multiple nodes in separate processes with the benefits of process/fault isolation as well as easier debugging of individual nodes and
- running multiple nodes in a single process with the lower overhead and optionally more efficient communication (see [Intra Process Communication](https://docs.ros.org/en/rolling/Tutorials/Demos/Intra-Process-Communication.html)).

## composition和execution的关系

有了composition，并不需要显式操作executor，用户不需要关注executor的使用<br />也减少了executor的使用，导致execution的高级特性比如调度设置无法进行配置<br />除非在launch或cmd中可以对executor进行详细配置<br />The [composition](https://github.com/ros2/demos/tree/rolling/composition) package contains a couple of different approaches on how to use components. The three most common ones are:

1. Start a ([generic container process](https://github.com/ros2/rclcpp/blob/rolling/rclcpp_components/src/component_container.cpp)) and call the ROS service [load_node](https://github.com/ros2/rcl_interfaces/blob/rolling/composition_interfaces/srv/LoadNode.srv) offered by the container. The ROS service will then load the component specified by the passed package name and library name and start executing it within the running process. Instead of calling the ROS service programmatically you can also use a [command line tool](https://github.com/ros2/ros2cli/tree/rolling/ros2component) to invoke the ROS service with the passed command line arguments
2. Create a [custom executable](https://github.com/ros2/demos/blob/rolling/composition/src/manual_composition.cpp) containing multiple nodes which are known at compile time. This approach requires that each component has a header file (which is not strictly needed for the first case).
3. Create a launch file and use ros2 launch to create a container process with multiple components loaded.

# 参考
<https://index.ros.org/doc/ros2/Tutorials/Intra-Process-Communication/><br /><https://design.ros2.org/articles/intraprocess_communications.html><br />[https://docs.ros.org/en/rolling/Concepts/About-Composition.html](https://docs.ros.org/en/rolling/Concepts/About-Composition.html)<br />[https://docs.ros.org/en/rolling/Tutorials/Demos/Intra-Process-Communication.html](https://docs.ros.org/en/rolling/Tutorials/Demos/Intra-Process-Communication.html)<br />shared_ptr智能指针<br />[https://blog.csdn.net/A_L_A_N/article/details/83378017](https://blog.csdn.net/A_L_A_N/article/details/83378017)
